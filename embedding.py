import json
import random
import numpy as np
from sentence_transformers import SentenceTransformer
from tqdm import tqdm

# --- 1. Cáº¤U HÃŒNH ---
INPUT_JSON = 'guardian_articles_ver2.json'
CONTENT_KEY = 'content'
MODEL_NAME = 'all-MiniLM-L6-v2'
OUTPUT_TXT = 'embeddings_ver2.txt'
OUTPUT_JSON = 'embeddings_ver2.json'
BATCH_SIZE = 64

# --- 2. Äáº¶T SEED CHO á»”N Äá»ŠNH ---
random.seed(42)
np.random.seed(42)

# --- 3. Táº¢I MÃ” HÃŒNH ---
print(f"Äang táº£i mÃ´ hÃ¬nh {MODEL_NAME}...")
model = SentenceTransformer(MODEL_NAME)
print("âœ… Táº£i mÃ´ hÃ¬nh thÃ nh cÃ´ng.")

# --- 4. Äá»ŒC FILE JSON ---
print(f"Äang Ä‘á»c file {INPUT_JSON}...")
try:
    with open(INPUT_JSON, 'r', encoding='utf-8') as f:
        articles_data = json.load(f)
    all_texts = [article.get(CONTENT_KEY, "") for article in articles_data]
    print(f"ÄÃ£ Ä‘á»c xong {len(all_texts)} bÃ i bÃ¡o.")
except Exception as e:
    print(f"âŒ Lá»—i khi Ä‘á»c file JSON: {e}")
    exit()

# --- 5. MÃƒ HÃ“A & LÆ¯U ---
print(f"ğŸ” Báº¯t Ä‘áº§u mÃ£ hÃ³a {len(all_texts)} vÄƒn báº£n...")
all_embeddings = []

with open(OUTPUT_TXT, 'w', encoding='utf-8') as f_out:
    for i in tqdm(range(0, len(all_texts), BATCH_SIZE), desc="Äang xá»­ lÃ½ cÃ¡c lÃ´"):
        batch_texts = all_texts[i: i + BATCH_SIZE]
        embeddings = model.encode(batch_texts, convert_to_numpy=True)
        for vec in embeddings:
            f_out.write(' '.join(map(str, vec)) + '\n')
            all_embeddings.append(vec.tolist())

# --- 6. LÆ¯U FILE JSON ---
print(f"ğŸ’¾ Äang lÆ°u dá»¯ liá»‡u ra {OUTPUT_JSON}...")
with open(OUTPUT_JSON, 'w', encoding='utf-8') as jf:
    json.dump(all_embeddings, jf, ensure_ascii=False, indent=2)

print(f"\nğŸ‰ HoÃ n táº¥t mÃ£ hÃ³a {len(all_texts)} vÄƒn báº£n.")
print(f"ğŸ“„ File TXT: {OUTPUT_TXT}")
print(f"ğŸ“˜ File JSON: {OUTPUT_JSON}")
